{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import context\n",
    "\n",
    "import numpy as np\n",
    "from evalpack.utils.torczon.torczon import solve\n",
    "from evalpack.metrics import nab\n",
    "from evalpack.metrics import mti\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoProfile():\n",
    "    import numpy as np\n",
    "    from evalpack.utils.torczon.torczon import solve\n",
    "    def __init__(self, x0, xmin, xmax, Nguess, Niter, initial_box_width=0.1, rbo_p: list = [0.2, 1]) -> None:\n",
    "        self.rbo_p = rbo_p\n",
    "        self.x0 = x0\n",
    "        self.xmin = xmin\n",
    "        self.xmax = xmax\n",
    "        self.Nguess = Nguess\n",
    "        self.Niter= Niter\n",
    "        self.initial_box_width = initial_box_width\n",
    "\n",
    "    def compute_approx_rbo(self, ranking, scores, p):\n",
    "        deltas = np.zeros(len(scores))\n",
    "        for i_algo in range(len(scores)):\n",
    "            deltas[i_algo] = sum([1 for j_algo in range(len(scores)) if (i_algo != j_algo) and (scores[i_algo] >= scores[j_algo])])\n",
    "        vals_gt = np.unique(ranking)\n",
    "        return sum([(p**i_val)*sum((ranking >= val_rank)*(deltas >= val_rank))/sum(ranking >= val_rank) for i_val, val_rank in enumerate(vals_gt)])\n",
    "\n",
    "    def compute_RBO_cost(self, ranking, scores):\n",
    "        return - sum([self.compute_approx_rbo(ranking, scores, iter_p)/self.compute_approx_rbo(ranking, ranking, iter_p) for iter_p in self.rbo_p])\n",
    "    \n",
    "\n",
    "    def objective_function(self, var, params):\n",
    "        scores = np.average(self.raw_scores, weights=var, axis=1)\n",
    "        return self.compute_RBO_cost(self.ranking, scores)\n",
    "\n",
    "    def fit(self, raw_ranking, raw_scores):\n",
    "        ranking = []\n",
    "        for pos in raw_ranking:\n",
    "            tmp_ranking = np.zeros(len(pos)) - 1\n",
    "            ranking = np.concatenate([ranking, tmp_ranking]) + len(pos)\n",
    "        self.ranking = ranking\n",
    "        self.raw_scores = raw_scores\n",
    "\n",
    "        self.solution = solve(self.objective_function, par=None, x0=self.x0, xmin=self.xmin, xmax=self.xmax, Nguess=self.Nguess, Niter=self.Niter, initial_box_width=self.initial_box_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ranking = [['A'], ['B'], ['C'], ['D']]\n",
    "raw_scores = [[0.9, 0.9, 0.7, 0.7], [0.9, 0.6, 0.8, 1], [0, 1, 0.5, 1], [0, 0, 0.3, 0.2]]\n",
    "\n",
    "auto_mti = AutoProfile(x0=[1,1,1,1], xmin=[0,0,0,0], xmax=[1,1,1,1], Nguess=20, Niter=50, rbo_p=[0.2, 1])\n",
    "\n",
    "auto_mti.fit(raw_ranking=raw_ranking, raw_scores=raw_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_wrapper(y_true, y_pred, threshold, true_ranking, metric='MTI', rbo_p=[0.2,1], Nguess=20, Niter=50):\n",
    "    if metric == 'MTI':\n",
    "        scores = np.zeros((y_pred.shape[1], 4))\n",
    "        metric = mti.MTI()\n",
    "        for i_algo in range(y_pred.shape[1]):\n",
    "            pred = np.zeros(y_pred.shape[0])\n",
    "            pred[y_pred[:, i_algo] > threshold] = 1\n",
    "            metric.compute_metrics(labels=y_true, predictions=pred, pos_label=1)\n",
    "            scores[i_algo, :] = [metric.recall_score, metric.masked_specificity_score, metric.alarm_cardinality_score, metric.anticipation_score,]\n",
    "        autop = AutoProfile(x0=[1,1,1,1], xmin=[0,0,0,0], xmax=[1,1,1,1], Nguess=Nguess, Niter=Niter, rbo_p=rbo_p)\n",
    "        autop.fit(raw_ranking=true_ranking, raw_scores=scores)\n",
    "        return scores, autop.solution.x, autop.solution.f\n",
    "    elif metric == 'NAB':\n",
    "        scores = np.zeros((y_pred.shape[1], 3))\n",
    "        for i_algo in range(y_pred.shape[1]):\n",
    "            pred = np.zeros(y_pred.shape[0])\n",
    "            pred[y_pred[:, i_algo] > threshold] = 1\n",
    "            scores[i_algo, :] = nab.nab_score(y_true=y_true, y_pred=pred, return_sum=False)\n",
    "        autop = AutoProfile(x0=[1,1,1], xmin=[0,0,0], xmax=[1,1,1], Nguess=Nguess, Niter=Niter, rbo_p=rbo_p)\n",
    "        autop.fit(raw_ranking=true_ranking, raw_scores=scores)\n",
    "        return scores, autop.solution.x, autop.solution.f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTI [0.60295072 0.45543117 0.47402513 0.01122133] -2.0 [0.63252648 0.62227654 0.49355086 0.48468408 0.37420065]\n",
      "NAB [1. 1. 1.] -1.6181818181818182 [-104.33779523    0.32881694    0.32557811   -1.750345   -177.58277899]\n"
     ]
    }
   ],
   "source": [
    "df_simple = pd.read_csv('../datasets/synthetic_simple_cases.csv')\n",
    "labels = df_simple.copy().labels.to_numpy(dtype=int)\n",
    "labels[labels == 1] = 0\n",
    "labels[labels == -1] = 1\n",
    "true_ranking = [['A'], ['B'], ['C'], ['D'], ['E']]\n",
    "scores, weights, cost = metric_wrapper(y_true=labels, y_pred=df_simple.to_numpy()[:, 1:], threshold=0.5, true_ranking=true_ranking, \n",
    "               metric='MTI', rbo_p=[0.2, 1], Nguess=20, Niter=50)\n",
    "print('MTI', weights, cost, np.average(scores, weights=weights, axis=1))\n",
    "scores, weights, cost = metric_wrapper(y_true=labels, y_pred=df_simple.to_numpy()[:, 1:], threshold=0.5, true_ranking=true_ranking, \n",
    "               metric='NAB', rbo_p=[0.2, 1], Nguess=20, Niter=50)\n",
    "print('NAB', weights, cost, np.average(scores, weights=weights, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTI [1.  1.  0.9 1. ] -1.98211110906311 [0.9933237  0.83419184 0.70598671 0.7052717  0.6167532  0.61538462\n",
      " 0.54537771 0.54522439 0.39316239 0.28176674]\n",
      "MTI with init weights: [0.99176485 0.97683276 0.80705287 0.90304667 0.68642186 0.79439366\n",
      " 0.64656686 0.64654544 0.4987433  0.32450721]\n",
      "NAB [0.91096929 0.86225105 0.00479952] -1.7459520614533812 [ 0.12130621  1.01098458  1.01098458  1.23064805  0.95103053  0.86923702\n",
      "  0.02054211  0.02054211 -0.28302335 -0.36546223]\n",
      "NAB with init weights: [-1.09205134e+02  6.57742865e-01  6.57742865e-01  3.14278727e+01\n",
      " -6.74574611e+00  5.65522427e-01 -4.46190062e-03 -4.46190062e-03\n",
      " -2.01960477e-01 -1.68813014e+02]\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('../datasets/synthetic_two_asymetric_areas.csv')\n",
    "labels = df2.copy().labels.to_numpy(dtype=int)\n",
    "labels[labels == 1] = 0\n",
    "labels[labels == -1] = 1\n",
    "order = ['perfect_score_with_anticipation', 'perfect_score', 'first_half_score', 'shift_100_perfect_score', 'perfect_score_with_FA', \n",
    "         'second_half_score', 'first_area_score', 'second_area_score', '1/5_first_area_score', 'random_score']\n",
    "true_ranking = [['perfect_score_with_anticipation'], ['perfect_score'], ['first_half_score'], ['shift_100_perfect_score'], \n",
    "                ['perfect_score_with_FA'], ['second_half_score'], ['first_area_score', 'second_area_score'], ['1/5_first_area_score'], \n",
    "                ['random_score']]\n",
    "scores, weights, cost = metric_wrapper(y_true=labels, y_pred=df2[order].to_numpy(), threshold=0.5, true_ranking=true_ranking, \n",
    "               metric='MTI', rbo_p=[0.2, 1], Nguess=20, Niter=50)\n",
    "print('MTI', weights, cost, np.average(scores, weights=weights, axis=1))\n",
    "print('MTI with init weights:', np.average(scores, weights=[0.990317,0.92242083,0.8992485,0.1044869 ], axis=1))\n",
    "scores, weights, cost = metric_wrapper(y_true=labels, y_pred=df2[order].to_numpy(), threshold=0.5, true_ranking=true_ranking, \n",
    "               metric='NAB', rbo_p=[0.2, 1], Nguess=20, Niter=50)\n",
    "print('NAB', weights, cost, np.average(scores, weights=weights, axis=1))\n",
    "print('NAB with init weights:', np.average(scores, weights=[1,1,1], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTI [0.37887798 0.62789045 0.50878114 0.53438756] -2.0 [0.98959604 0.8353858  0.7429737  0.66451847 0.66352959 0.64690305\n",
      " 0.57084161 0.30629742]\n",
      "MTI with init weights: [0.98925698 0.97737673 0.80759684 0.82745438 0.76194005 0.79439366\n",
      " 0.64682814 0.31627955]\n",
      "NAB [2.81669642e-01 6.93361216e-01 2.61626058e-04] -1.7391421588705933 [ 3.89921259  3.98915219  3.98915219  3.53285512  3.98293076  3.4298434\n",
      " -2.981909   -9.95297019]\n",
      "NAB with init weights: [-1.07155069e+02  4.60420006e+00  4.60420006e+00 -1.53471715e+02\n",
      " -3.12656675e+00  3.95865699e+00 -3.12333043e-02 -4.66666667e+00]\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv('../datasets/synthetic_many_small_areas.csv')\n",
    "labels = df3.copy().labels.to_numpy(dtype=int)\n",
    "labels[labels == 1] = 0\n",
    "labels[labels == -1] = 1\n",
    "order = ['perfect_score_with_anticipation', 'perfect_score', 'perfect_score_first_half_each_areas', 'shift_100', 'perfect_score_with_FA',\n",
    "         'perfect_score_second_half_each_areas', 'perfect_score_half_areas', 'random_score']\n",
    "true_ranking = [['perfect_score_with_anticipation'], ['perfect_score'], ['perfect_score_first_half_each_areas'], ['shift_100'], \n",
    "                ['perfect_score_with_FA'], ['perfect_score_second_half_each_areas'], ['perfect_score_half_areas'], ['random_score']]\n",
    "scores, weights, cost = metric_wrapper(y_true=labels, y_pred=df3[order].to_numpy(), threshold=1, true_ranking=true_ranking, \n",
    "               metric='MTI', rbo_p=[0.2, 1], Nguess=20, Niter=50)\n",
    "print('MTI', weights, cost, np.average(scores, weights=weights, axis=1))\n",
    "print('MTI with init weights:', np.average(scores, weights=[0.990317,0.92242083,0.8992485,0.1044869 ], axis=1))\n",
    "scores, weights, cost = metric_wrapper(y_true=labels, y_pred=df3[order].to_numpy(), threshold=1, true_ranking=true_ranking, \n",
    "               metric='NAB', rbo_p=[0.2, 1], Nguess=20, Niter=50)\n",
    "print('NAB', weights, cost, np.average(scores, weights=weights, axis=1))\n",
    "print('NAB with init weights:', np.average(scores, weights=[1,1,1], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
